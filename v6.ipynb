{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8LuOAeaPySU"
      },
      "source": [
        "### __Imports and installs__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5TRx58E1BEU",
        "outputId": "e0da2a16-1d0d-4343-ac8f-854f8471cac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (9.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.16.0+cu121)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->grad-cam) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "fatal: destination path 'tf_to_pytorch_model' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install grad-cam\n",
        "!pip install timm\n",
        "!git clone https://github.com/ylhz/tf_to_pytorch_model.git\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "import skimage\n",
        "from skimage.feature import peak_local_max\n",
        "\n",
        "import timm\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HmSIJ3Jp7L2g"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xrD68AfQAke"
      },
      "source": [
        "### __Dataset__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HaLWE-tFBnHu"
      },
      "outputs": [],
      "source": [
        "def read_images_and_labels(images_folder_path, csv_file_path):\n",
        "    labels_df = pd.read_csv(csv_file_path)\n",
        "    labels_dict = pd.Series(labels_df.TrueLabel.values, index=labels_df.ImageId).to_dict()\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(images_folder_path):\n",
        "        if filename.endswith(\".png\") and filename != \"362e4ac62cf888f4.png\" and filename != \"bd3617fcc985fe31.png\":\n",
        "            image_id = filename.replace('.png', '')\n",
        "            img_path = os.path.join(images_folder_path, filename)\n",
        "\n",
        "            img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "            images.append(img)\n",
        "\n",
        "            if image_id in labels_dict:\n",
        "                labels.append(labels_dict[image_id])\n",
        "            else:\n",
        "                labels.append(None)  # (should not happen)\n",
        "\n",
        "    images_array = np.array(images)\n",
        "    labels_array = np.array(labels)\n",
        "\n",
        "    return images_array, labels_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOxYuFUBxaQ",
        "outputId": "15d27498-fde9-42c9-e8c0-a3ce5a07f7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(998, 299, 299, 3) (998,)\n"
          ]
        }
      ],
      "source": [
        "folder_path = 'tf_to_pytorch_model/dataset/images'\n",
        "csv_file_path = 'tf_to_pytorch_model/dataset/dev_dataset.csv'\n",
        "X, y = read_images_and_labels(folder_path, csv_file_path)\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "y -= 1\n",
        "y_long = torch.tensor(y, dtype=torch.long)\n",
        "one_hot_y = F.one_hot(y_long, num_classes = 1000)\n",
        "one_hot_y = one_hot_y.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Nhbh3LLJMivp"
      },
      "outputs": [],
      "source": [
        "# Load ImageNet class labels\n",
        "LABELS_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'\n",
        "labels_response = requests.get(LABELS_URL)\n",
        "labels_dict = labels_response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NGCiTVpu1BEV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "for i in range(24):\n",
        "    plt.subplot(4, 6, i + 1)\n",
        "    plt.imshow(X[i])\n",
        "    plt.title(labels_dict[str(y[i])][1])\n",
        "    plt.axis('off')\n",
        "plt.gcf().tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDEC_VpbQF3O"
      },
      "source": [
        "### __Preprocessing__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B04YFXEc1BEX"
      },
      "outputs": [],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "def resize_image(input_images, model_name):\n",
        "    model_shapes = {\n",
        "        \"squeezenet\": (224, 224),\n",
        "        \"inceptionv4\": (299, 299),\n",
        "        \"resnet50\": (224, 224),\n",
        "    }\n",
        "\n",
        "    target_shape = model_shapes.get(model_name)\n",
        "\n",
        "    if target_shape is None:\n",
        "        raise ValueError(\"Invalid model name. Please provide a valid model name.\")\n",
        "\n",
        "    # Check if input is a batch of images (4-dimensional)\n",
        "    if len(input_images.shape) == 4:\n",
        "        resized_images = []\n",
        "        for img in input_images:\n",
        "            # Resize each image in the batch\n",
        "            resized_img = skimage.transform.resize(img, (*target_shape, 3), anti_aliasing=True)\n",
        "            resized_images.append(resized_img)\n",
        "        resized_images = np.array(resized_images)\n",
        "        return resized_images\n",
        "    # If input is a single image (3-dimensional)\n",
        "    elif len(input_images.shape) == 3:\n",
        "        resized_image = skimage.transform.resize(input_images, (*target_shape, 3), anti_aliasing=True)\n",
        "        return resized_image\n",
        "    else:\n",
        "        raise ValueError(\"Input images should have 3 or 4 dimensions.\")\n",
        "\n",
        "class Preprocessing_Transform:\n",
        "    def __init__(self, mean, std, model_name):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = resize_image(img, self.model_name)\n",
        "        if len(img.shape)==3:\n",
        "          img_normalized = (img - np.array(self.mean)[None, None, :]) / np.array(self.std)[None, None, :] # Normalize\n",
        "          return torch.tensor(img_normalized, dtype=torch.float32).permute(2, 0, 1)\n",
        "        elif len(img.shape)==4:\n",
        "          img_normalized = (img - np.array(self.mean)[None, None, None, :]) / np.array(self.std)[None, None, None, :] # Normalize\n",
        "          return torch.tensor(img_normalized, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "class Depreprocessing_Transform:\n",
        "    def __init__(self, mean, std, model_name):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, tensor_img):\n",
        "        if len(tensor_img.shape)==3:\n",
        "          img_normalized = tensor_img.permute(1,2,0).detach().cpu().numpy()\n",
        "          img = (img_normalized * np.array(self.std)[None, None, :]) + np.array(self.mean)[None, None, :] # Denormalize\n",
        "          img = resize_image(img, self.model_name)\n",
        "          return img\n",
        "        elif len(tensor_img.shape)==4:\n",
        "          img_normalized = tensor_img.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "          img = (img_normalized * np.array(self.std)[None, None, None, :]) + np.array(self.mean)[None, None, None, :] # Denormalize\n",
        "          img = resize_image(img, self.model_name)\n",
        "          return img\n",
        "\n",
        "preprocess_squeezenet = Preprocessing_Transform(mean, std, \"squeezenet\")\n",
        "depreprocess_squeezenet = Depreprocessing_Transform(mean, std, \"squeezenet\")\n",
        "\n",
        "preprocess_inceptionv4 = Preprocessing_Transform(mean, std, \"inceptionv4\")\n",
        "depreprocess_inceptionv4 = Depreprocessing_Transform(mean, std, \"inceptionv4\")\n",
        "\n",
        "preprocess_resnet50 = Preprocessing_Transform(mean, std, \"resnet50\")\n",
        "depreprocess_resnet50 = Depreprocessing_Transform(mean, std, \"resnet50\")\n",
        "\n",
        "class Normalized_Clamp:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = torch.Tensor(mean)\n",
        "        self.std = torch.Tensor(std)\n",
        "\n",
        "    def __call__(self, normalized_tensor):\n",
        "        low = (-self.mean/self.std).view(3,1,1).expand_as(normalized_tensor)\n",
        "        high = ((1-self.mean)/self.std).view(3,1,1).expand_as(normalized_tensor)\n",
        "        return torch.clamp(normalized_tensor, low, high)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbH59eeoQMBP"
      },
      "source": [
        "### __Models__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3b8nHXfgxAN"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "resnet50_model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "resnet50_model.to(device)\n",
        "\n",
        "# Freeze the weights\n",
        "for param in resnet50_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Model in test mode\n",
        "resnet50_model.eval()\n",
        "\n",
        "# Load the model\n",
        "squeezenet_model = torchvision.models.squeezenet1_1(weights=torchvision.models.SqueezeNet1_1_Weights.DEFAULT)\n",
        "squeezenet_model.to(device)\n",
        "\n",
        "# Freeze the weights\n",
        "for param in squeezenet_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Model in test mode\n",
        "squeezenet_model.eval()\n",
        "\n",
        "inceptionv4_model = timm.create_model('inception_v4', pretrained=True)\n",
        "inceptionv4_model.to(device)\n",
        "\n",
        "# Freeze the weights\n",
        "for param in inceptionv4_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Model in test mode\n",
        "inceptionv4_model.eval()\n",
        "\n",
        "# Load the model\n",
        "adv_inceptionv3_model = timm.create_model('adv_inception_v3', pretrained=True)\n",
        "adv_inceptionv3_model.to(device)\n",
        "\n",
        "# Freeze the weights\n",
        "for param in adv_inceptionv3_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Model in test mode\n",
        "adv_inceptionv3_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkg3SIm5lK6q"
      },
      "outputs": [],
      "source": [
        "for name, module in adv_inceptionv3_model.named_modules():\n",
        "  if '.' not in name:\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60z-oSTcKp0j"
      },
      "outputs": [],
      "source": [
        "# Organize in dictionnary\n",
        "models = {\"resnet50\": resnet50_model,\n",
        "          \"squeezenet\": squeezenet_model,\n",
        "          \"inceptionv4\": inceptionv4_model,\n",
        "          \"adv_inceptionv3\": adv_inceptionv3_model}\n",
        "\n",
        "# Set target layers\n",
        "target_layers = {\"resnet50\": resnet50_model.layer4[-1],\n",
        "                 \"squeezenet\": squeezenet_model.features[-1],\n",
        "                 \"inceptionv4\": inceptionv4_model.features[-1],\n",
        "                 'adv_inceptionv3': adv_inceptionv3_model.Mixed7c}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp65VzF1QRUP"
      },
      "source": [
        "### __Class Activation Maps__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWPykD_W1BEX"
      },
      "outputs": [],
      "source": [
        "targets = [ClassifierOutputTarget(class_id) for class_id in y]\n",
        "\n",
        "input_tensor_squeezenet = preprocess_squeezenet(X[:25]).detach().requires_grad_(True)\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam_squeezenet = GradCAMPlusPlus(model=squeezenet_model, target_layers=[target_layers[\"squeezenet\"]])\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_squeezenet = cam_squeezenet(input_tensor=input_tensor_squeezenet, targets=targets)\n",
        "\n",
        "\n",
        "\n",
        "input_tensor_inceptionv4 = preprocess_inceptionv4(X[:25]).detach().requires_grad_(True)\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam_inceptionv4 = GradCAMPlusPlus(model=inceptionv4_model, target_layers=[target_layers[\"inceptionv4\"]])\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_inceptionv4 = cam_inceptionv4(input_tensor=input_tensor_inceptionv4, targets=targets)\n",
        "\n",
        "\n",
        "\n",
        "input_tensor_resnet50 = preprocess_resnet50(X[:25]).detach().requires_grad_(True)\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam_resnet50 = GradCAMPlusPlus(model=resnet50_model, target_layers=[target_layers[\"resnet50\"]])\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_resnet50 = cam_resnet50(input_tensor=input_tensor_resnet50, targets=targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbpKXxu21BEX"
      },
      "outputs": [],
      "source": [
        "def print_cams(grayscale_cams, model_name, images):\n",
        "\n",
        "  resized_images = resize_image(images, model_name)\n",
        "\n",
        "  grayscale_cams = [cam.squeeze() for cam in grayscale_cams]  # Removing any extra dimensions\n",
        "\n",
        "  num_images_to_display = 25\n",
        "\n",
        "  columns = 5  # Number of columns in the grid\n",
        "  rows = math.ceil(num_images_to_display / columns)  # Calculate the number of rows needed\n",
        "\n",
        "  fig, axes = plt.subplots(rows, columns, figsize=(15, rows * 3))  # Adjust figsize as needed\n",
        "  axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
        "\n",
        "  for i in range(num_images_to_display):\n",
        "      if i < len(images):\n",
        "\n",
        "          img = resized_images[i]\n",
        "\n",
        "          # Get the corresponding CAM\n",
        "          cam_image = grayscale_cams[i]\n",
        "\n",
        "          # Overlay CAM on the image\n",
        "          visualization = show_cam_on_image(img, cam_image, use_rgb=True)\n",
        "\n",
        "          ax = axes[i]\n",
        "          ax.imshow(visualization)\n",
        "          ax.axis('off')\n",
        "      else:\n",
        "          break  # Break if there are no more images\n",
        "\n",
        "  # Hide any unused axes if there are fewer images than subplots\n",
        "  for ax in axes[num_images_to_display:]:\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OycsWNX5IDd7"
      },
      "outputs": [],
      "source": [
        "print_cams(grayscale_cam_squeezenet, \"squeezenet\", X[:25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ctb9rs_IGV2"
      },
      "outputs": [],
      "source": [
        "print_cams(grayscale_cam_inceptionv4, \"inceptionv4\", X[:25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3w8Sps4seFB"
      },
      "outputs": [],
      "source": [
        "print_cams(grayscale_cam_resnet50, \"resnet50\", X[:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ZRn-R1QYEt"
      },
      "source": [
        "### __Local Maxima__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHdMXqppGwPz"
      },
      "outputs": [],
      "source": [
        "def get_centers(M, ratio_threshold = 0.6, min_distance = 20):\n",
        "    # Find coordinates of local maxima\n",
        "    coordinates = peak_local_max(M, num_peaks=3, exclude_border=False, min_distance=min_distance)\n",
        "\n",
        "    if len(coordinates)==0:\n",
        "      global_max_index = np.unravel_index(np.argmax(M), M.shape)\n",
        "      return np.array([global_max_index])\n",
        "\n",
        "    # Extract the values at these coordinates\n",
        "    maxima_values = M[coordinates[:, 0], coordinates[:, 1]]\n",
        "\n",
        "    # Determine the highest value among the maxima\n",
        "    highest_maxima_value = max(maxima_values)\n",
        "\n",
        "    # Filter coordinates based on the threshold\n",
        "    filtered_coordinates = coordinates[maxima_values >= ratio_threshold * highest_maxima_value]\n",
        "    return filtered_coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8hSisVBGeWa"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    M = grayscale_cam_squeezenet[i]\n",
        "\n",
        "    # Plot the grayscale cam\n",
        "    ax.imshow(M, cmap='gray')\n",
        "\n",
        "    filtered_coordinates = get_centers(M, ratio_threshold = 0.6, min_distance = 20)\n",
        "\n",
        "    # Plot the filtered coordinates on the images\n",
        "    ax.plot(filtered_coordinates[:, 1], filtered_coordinates[:, 0], 'r.')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jga49x-QcFt"
      },
      "source": [
        "### __Main algorithm__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ks3esWX3PTI"
      },
      "outputs": [],
      "source": [
        "def ADP(x_clean, y_true, model_name, ratio_threshold, min_distance, eps, T, alpha, beta, m, mean, std, cam_method = \"normal\", momentum = None):\n",
        "\n",
        "  if momentum != None:\n",
        "    use_momentum = True\n",
        "  else:\n",
        "    use_momentum = False\n",
        "\n",
        "  model = models[model_name]\n",
        "\n",
        "  target_layer = [target_layers[model_name]]\n",
        "\n",
        "  if cam_method == \"normal\":\n",
        "    cam = GradCAM(model=model, target_layers=target_layer)\n",
        "\n",
        "  elif cam_method == \"++\":\n",
        "    cam = GradCAMPlusPlus(model=model, target_layers=target_layer)\n",
        "\n",
        "  targets = [ClassifierOutputTarget(np.argmax(y_true))]\n",
        "\n",
        "  x_adv = torch.clone(x_clean).detach().requires_grad_(True)\n",
        "\n",
        "  W, H = x_clean.shape[1], x_clean.shape[2]\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  normalized_clamp = Normalized_Clamp(mean, std)\n",
        "\n",
        "  for t in range(0, T):\n",
        "    x_adv.requires_grad_(True)\n",
        "    g = 0\n",
        "    if use_momentum:\n",
        "      previous_g = 0\n",
        "    M = cam(input_tensor=x_adv, targets=targets)\n",
        "\n",
        "    centers = get_centers(M[0], ratio_threshold, min_distance)\n",
        "\n",
        "    for center in centers:\n",
        "\n",
        "      for k in range(1, m+1):\n",
        "        x1 = int(max(center[0]-beta*k, 0))\n",
        "        x2 = int(min(center[0]+beta*k, W))\n",
        "        y1 = int(max(center[1]-beta*k, 0))\n",
        "        y2 = int(min(center[1]+beta*k, H))\n",
        "\n",
        "        x_drop = torch.clone(x_adv).detach()\n",
        "        x_drop.data[:, x1:x2, y1:y2] = x_clean.data[:, x1:x2, y1:y2]\n",
        "        x_drop = x_drop.detach()\n",
        "        x_drop.requires_grad_(True)\n",
        "\n",
        "        loss = criterion(model(x_drop)[0], y_true)\n",
        "        loss.backward()\n",
        "\n",
        "        grad = x_drop.grad.data\n",
        "        g += grad\n",
        "\n",
        "        x_drop.grad.zero_()\n",
        "\n",
        "    g *= 1/(len(centers)*m)\n",
        "\n",
        "    if use_momentum:\n",
        "      g = momentum * previous_g + g / torch.mean(torch.abs(g), dim = (1,2,3), keepdim=True)\n",
        "\n",
        "    x_adv_max = x_clean + eps\n",
        "    x_adv_min = x_clean - eps\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      x_adv_max = normalized_clamp(x_adv_max)\n",
        "      x_adv_min = normalized_clamp(x_adv_min)\n",
        "\n",
        "      g_sign = g.sign()\n",
        "      perturbed_x_adv = x_adv + alpha * g_sign\n",
        "      x_adv = torch.max(torch.min(perturbed_x_adv, x_adv_max), x_adv_min)\n",
        "\n",
        "      previous_g = torch.clone(g)\n",
        "\n",
        "  return x_adv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDJbdvTvS-oe"
      },
      "source": [
        "#### Example use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZwSmOvILiNG"
      },
      "outputs": [],
      "source": [
        "x_clean = preprocess_squeezenet(X[0]).unsqueeze(0)\n",
        "y_true = one_hot_y[0]\n",
        "\n",
        "x_adv = ADP(x_clean, y_true, \"squeezenet\", ratio_threshold=0.6, min_distance=20, eps=0.274, T=10, alpha=0.5, beta=15, m=5, mean=mean, std=std, cam_method = \"++\", momentum = 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IOfNQlXeREs"
      },
      "outputs": [],
      "source": [
        "transfer_x_adv = preprocess_inceptionv4(depreprocess_squeezenet(x_adv.squeeze()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCJ37cVQog9A"
      },
      "outputs": [],
      "source": [
        "y_pred_squeezenet_before = np.argmax(torch.nn.functional.softmax(models[\"squeezenet\"](x_clean)[0], dim=0))\n",
        "y_pred_inceptionv4_before = np.argmax(torch.nn.functional.softmax(models[\"inceptionv4\"](x_clean)[0], dim=0))\n",
        "\n",
        "y_pred_squeezenet_after = np.argmax(torch.nn.functional.softmax(models[\"squeezenet\"](x_adv)[0], dim=0))\n",
        "y_pred_inceptionv4_after = np.argmax(torch.nn.functional.softmax(models[\"inceptionv4\"](transfer_x_adv.unsqueeze(0))[0], dim=0))\n",
        "\n",
        "print(\"True class: \", labels_dict[str(int(np.argmax(one_hot_y[0])))][1])\n",
        "\n",
        "print(\"Initial predicted class, Attack model: \", labels_dict[str(int(y_pred_squeezenet_before))][1])\n",
        "print(\"Initial predicted class, Transfer model: \", labels_dict[str(int(y_pred_inceptionv4_before))][1])\n",
        "\n",
        "print(\"New predicted class, Attack model: \", labels_dict[str(int(y_pred_squeezenet_after))][1])\n",
        "print(\"New predicted class, Transfer model: \", labels_dict[str(int(y_pred_inceptionv4_after))][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm30_nUwTwBk"
      },
      "source": [
        "### __Statistics__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cijom1V8VAnU"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model_name, dataset_X, dataset_y):\n",
        "  model = models[model_name]\n",
        "  with torch.no_grad():\n",
        "    y_pred = np.array(model(dataset_X).argmax(dim=1))\n",
        "\n",
        "  accuracy = np.mean((y_pred == dataset_y))\n",
        "\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GehVIzKOasFS"
      },
      "outputs": [],
      "source": [
        "get_accuracy(\"squeezenet\", preprocess_squeezenet(X), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ftjd0QJurkH"
      },
      "source": [
        "TODO : clamp avec bonnes valeurs et adapter eps en conséquence (done!); chercher les 1000 images et gérer le pb de la bande en bas (done!); chercher autres modèles (cf papier), Essayer moyenner x_adv sur différents modèles\n",
        "IDEES AMELIO : rescale dans algo (done), gradCAM (RDFIA) en + de gradCAM++; threshold au lieu de région carrée pour régions dropout, tester en mettant le gradient à 0 dans les régions dropped out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wMR3pvwUcwx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "J8LuOAeaPySU",
        "0xrD68AfQAke",
        "SDEC_VpbQF3O",
        "yp65VzF1QRUP",
        "j3ZRn-R1QYEt",
        "4jga49x-QcFt"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepdac",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}